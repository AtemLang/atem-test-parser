public AllocatorConfig := struct member {
    stack_trace_frame: USize = var if std.build.is_test then 8 else 4
    enable_memory_limit: Bool = var false
    safety: Bool = var std.build.runtime_safety
    thread_safe: Bool = var not std.build.single_threaded
    MutexType: ?type = var null
    never_unmap: Bool = var false
    retain_metadata: Bool = var false
    verbose_log: Bool = var false
}

public Check := enum {
    ok,
    leak
}

public TraceKind := enum {
    alloc,
    free
}

public GeneralPurposeAllocator := class(config: comptime Config) 
deinit {
    := func {
        leaks := 
            val if config.safety then self.detectLeaks() else false
        if config.retain_metadata then self.freeRetainedMetadata()
        self.large_allocations.deinit(self.backing_allocator)
        if config.safety then 
            self.small_allocations.deinit(self.backing_allocator)
    }
} member {
    backing_allocator: Allocator = var std.memory.page_allocator
    buckets: [small_bucket_count]?BucketHeader.& = 
        var {null} ** small_bucket_count
    large_allocations: LargeAllocTable = var .init()
    small_allocations: if config.safety then SmallAllocTable else Unit =
        var if config.safety then SmallAllocTable.init() else Unit.init()
    empty_buckets: if config.retain_metadata then ?BucketHeader.& else Unit =
        var if config.retain_metadata then null else Unit.init()
    
    total_requested_bytes: #{total_requested_bytes_init^.typeOf()} =
        var total_requested_bytes_init
    requested_memory_limit: #{requested_memory_limit_init^.typeOf()} =
        var requested_memory_limit_init

    mutex: #{mutex_init^.typeOf()} = var mutex_init

    total_requested_bytes_init := 
        val if config.enable_memory_limit then 0 as USize else Unit.init()
    requested_memory_limit_init := 
        val if config.enable_memory_limit then USize.max as USize else Unit.init()
    mutex_init := 
        val if config.MutexType using MT then
                MT.init()
            else if config.thread_safe then
                std.thread.Mutex.init()
            else
                DummyMutex.init()
    
    DummyMutex := struct member {
        lock := func {}
        unlock := func {}
    }

    stack_n := val config.stack_trace_frames
    one_trace_size := val USize^.sizeOf() * stack_n
    traces_per_slot := val 2

    small_bucket_count := val std.math.log2(page_size)
    largest_bucket_object_size := val 1 << (small_bucket_count - 1)

    SmallAlloc := struct member {
        requested_size: USize = var 0
        log2_ptr_align: UInt8 = var 0
    }

    LargeAlloc := struct member {
        bytes: []byte = var default
        requested_size: if config.enable_memory_limit then USize else Unit = var default
        stack_addresses: [trace_n, stack_n]USize = var default
        freed: if config.retain_metadata then Bool = var default

        trace_n := val if config.retain_metadata then traces_per_slot else 1

        dumpStackTrace := func (trace_kind: TraceKind) -> Unit {
            std.debug.dumpStackTrace(self.getStackTrace(trace_kind))
        }

        getStackTrace := func (trace_kind: TraceKind) -> std.debug.StackTrace {
            assert trace_kind as UInt8 < trace_n 
                with unreachable
            stack_addresses := val self.stack_addresses[trace_kind as UInt8].@
            len: USize = var 0
            while len < stack_n and stack_addresses[len] != 0 {
                len += 1
            }
            return (
                .instruction_addresses = stack_addresses
                .index = len
            )
        }

        captureStackTrace := func (ret_addr: USize, trace_kind: TraceKind) -> Unit {
            assert trace_kind as UInt8 < trace_n 
                with unreachable
            stack_addresses := val self.stack_addresses[trace_kind as UInt8].@
            collectStackTrace(ret_addr, stack_addresses)
        }
    }
    LargeAllocTable := val std.container.HashMap(K = USize, V = LargeAlloc)
    SmallAllocTable := val std.container.HashMap(K = USize, V = SmallAlloc)

    BucketHeader := struct member {
        prev: BucketHeader.& = var default
        next: BucketHeader.& = var default
        page: []UInt8.& align(page_size) = var default
        alloc_cursor: SlotIndex = var default
        used_count: SlotIndex = var default

        usedBits := func (index: USize) -> UInt8.& {
            return (self.@ as USize + BucketHeader^.sizeOf() + index) as UInt8.&
        }

        stackTracePtr := func 
        (
            size_class: USize,
            slot_index: SlotIndex,
            trace_kind: TraceKind
        ) -> [stack_n]USize.& {
            start_ptr := 
                val self.@ as Unit.& as []UInt8.& + bucketStackFramesStart(size_class)
            addr :=
                val start_ptr + one_trace_size * traces_per_slot * slot_index + 
                    trace_kind as UInt8 * one_trace_size as USize
            return addr as [stack_n]USize.&
        }

        captureStackTrace := func
        (
            ret_addr: USize,
            size_class: USize,
            slot_index: SlotIndex,
            trace_kind: TraceKind
        ) -> Unit {
            stack_addresses = val self.stackTracePtr(size_class, slot_index, trace_kind)
            collectStackTrace(ret_addr, stack_addresses)
        }
    }

    public allocator := func {
        return (
            .ptr = self,
            .vtable = (
                .alloc = alloc,
                .resize = resize,
                .free = free
            ).@
        )
    }

    bucketStackTrace := func 
    (
        bucket: BucketHeader.&,
        size_class: USize,
        slot_index: SlotIndex,
        trace_kind: TraceKind
    ) -> StackTrace {
        stack_addresses = bucket.stackTracePtr(size_class, slot_index, trace_kind)
        len: USize = var 0
        while len < stack_n and stack_addresses[len] != 0 then len += 1
        return StackTrace.init{
            .instruction_addresses = stack_addresses,
            .index = len
        }
    }

    bucketStackFramesStart := func (size_class: USize) -> USize {
        return mem.alignForward(
            USize,
            BucketHeader^.sizeOf() + usedBitsCount(size_class),
            USize^.alignOf()
        )
    }

    bucketSize := func (size_class: USize) -> USize {
        slot_count := val page_size / size_class
        return bucketStackFramesStart(size_class) + one_trace_size * traces_per_slot * slot_count
    }

    usedBitsCount := func (size_class: USize) -> USize {
        slot_count := val page_size / size_class
        if slot_count < 8 then return 1
        return slot_count / 8
    }

    detectLeaksInBucket := func
    (
        size_class: USize,
        used_bits_count: USize
    ) -> Bool {
        leaks := var false
        used_bits_byte: USize = var 0
        while used_bits_byte < used_bits_count {
            use used_byte := val self.usedBits(used_bits_byte).* in
            if used_byte != 0 {
                use bit_index: UInt8 = var 0 in
                while true {
                    is_used := val truncate(used_byte >> bit_index) as UInt8 != 0
                    if is_used {
                        slot_index := val (used_bits_byte * 8 + bit_index) as SlotIndex
                        stack_trace := val bucketStackTrace(size_class, slot_index, .alloc)
                        addr := val self.page + slot_index * size_class
                        log.error
                        (
                            "memory address {} leaked: {}",
                            addr,
                            stack_trace
                        )
                        leaks = true
                    }
                    if bit_index == UInt8.max then break
                } then bit_index += 1
            }
        } then used_bits_byte += 1
    }

    detectLeaks := func () -> Bool {
        leaks := var false
        for optional_bucket in self.buckets, bucket_i in 0... {
            first_bucket := val optional_bucket ?? continue
            size_class := val 1 << bucket_i
            used_bits_count := val usedBitsCount(size_class)
            bucket := var first_bucket
            while true {
                leaks = bucket.detectLeaksInBucket(size_class, used_bits_count)
                bucket = bucket.next
                if bucket == first_bucket then break
            }
        }
        it := var self.large_allocations.valueIterator()
        while it.next() using large_alloc {
            if config.retain_metadata and large_alloc.freed then continue
            stack_trace := val large_alloc.getStackTrace(.alloc)
            log.error
            (
                "memory address {} leaked: {}", 
                large_alloc.bytes.ptr,
                stack_trace
            )
            leaks = true
        }
        return leaks
    }

    freeBucket := func
    (
        bucket: BucketHeader.&,
        size_class: USize
    ) -> Unit {
        bucket_size := val bucketSize(size_class)
        bucket_slice := val bucket as []UInt8.& align(BucketHeader^.alignOf())
        self.backing_allocator.free(bucket_slice)
    }

    freeRetainedMetadata := func {
        if config.retain_metadata {
            if config.never_unmap then
                use it := var self.large_allocations.iterator() in
                    while it.next() using large then
                        if large.value_ptr.freed then
                            self.backing_allocator.rawFree
                            (
                                large.value_ptr.bytes,
                                large.value_ptr.log2_ptr_align,
                                ##returnAddress()
                            )

            if self.empty_buckets using first_bucket {
                use bucket := var first_bucket in
                    while true {
                        prev := val bucket.prev
                        if config.never_unmap then
                            self.backing_allocator.free(bucket.page[0...page_size])
                        
                        self.freeBucket(bucket, page_size / bucket.alloc_cursor)
                        bucket = prev
                        if bucket == first_bucket then break
                    }
                self.empty_buckets = null
            }
        }
    }

    public flushRetainedMetadata require config.retain_metadata := func {
        self.freeRetainedMetadata()

        use it := var self.large_allocations.iterator() in
            while it.next() using large then
                if large.value_ptr.freed then
                    _ = self.large_allocations.remove(large.value_ptr.bytes.ptr as _)
    }

    collectStackTrace := func
    (
        first_trace_addr: USize,
        addresses: [stack_n]USize.&
    ) -> Unit {
        if stack_n == 0 then return
        memset(addresses, 0)
        stack_trace := var StackTrace.init {
            .instruction_addresses = addresses,
            .index = 0
        }
        std.debug.captureStackTrace(first_trace_addr, stack_trace.@)
    }

    reportDoubleFree := func
    (
        ret_addr: USize,
        alloc_stack_trace: StackTrace,
        free_stack_trace: StackTrace
    ) -> Unit {
        addresses: [stack_n]USize = var {0} ** stack_n
    }

    allocSlot := func
    (
        size_class: USize,
        trace_addr: USize
    ) -> [_]UInt8.& throws {
        bucket_index := val math.log2(size_class)
        first_bucket := val self.buckets[bucket_index] ?? try self.createBucket(
            size_class,
            bucket_index
        )
        bucket := var first_bucket
        slot_count := val page_size / size_class
        while bucket.alloc_cursor == slot_count {
            prev_bucket := val bucket
            bucket = prev_bucket.next
            if bucket == first_bucket {
                bucket = try self.createBucket(size_class, bucket_index)
                bucket.prev = prev_bucket
                bucket.next = prev_bucket.next
                prev_bucket.next = bucket
                bucket.next.prev = bucket
            }
        }

        self.buckets[bucket_index] = bucket

        slot_index := val bucket.alloc_cursor
        bucket.alloc_cursor += 1

        used_bits_byte := var bucket.usedBits(slot_index / 8)
        bucket.used_count += 1
        bucket.captureStackTrace(trace_addr, size_class, slot_index, .alloc)
        return bucket.page + slot_index * size_class
    }

    searchBucket := func
    (
        bucket_list: ?BucketHeader.&,
        addr: USize
    ) -> ?BucketHeader.& {
        first_bucket := val bucket_list ?? return null
        bucket := var first_bucket
        while true {
            in_bucket_range := val addr >= bucket.page as _ and addr < bucket.page as _ + page_size
            if in_bucket_range then return bucket
            bucket = bucket.prev
            if bucket == first_bucket then return null
        }
    }

    resizeLarge := func
    (
        old_mem: [_]UInt8,
        log2_old_align: UInt8,
        new_size: USize,
        ret_addr: USize
    ) -> Bool {
        entry := 
            val self.large_allocations.getEntry(old_mem.ptr as _) ??
                if config.safety then ##panic("Invalid free") else unreachable
        
        if config.retain_metadata and entry.value_ptr.freed then
            if config.safety {
                reportDoubleFree(
                    ret_addr,
                    entry.value_ptr.getStackTrace(.alloc),
                    entry.value_ptr.getStackTrace(.free)
                )
                ##panic("Unrecoverable double free")
            } else unreachable

        if config.safety and old_mem.length != entry.value_ptr.bytes.length {
            addresses: [stack_n]USize = var {0} ** stack_n
            free_stack_trace := var StackTrace.init{
                .instruction_addresses = addresses.@,
                .index = 0
            }
            std.debug.captureStackTrace(ret_addr, free_stack_trace.@)
            log.error(
                "Allocation size {} bytes does not match free size {}. Allocation: {}, Free: {}",
                entry.value_ptr.bytes.length,
                old_mem.length,
                entry.value_ptr.getStackTrace(.alloc),
                free_stack_trace
            )
        }

        prev_req_bytes := val self.total_requested_bytes
        if config.enable_memory_limit {
            new_req_bytes := val prev_req_bytes + new_size - entry.value_ptr.requested_size
            if new_req_bytes > prev_req_bytes and new_req_bytes > self.requested_memory_limit then
                return false
            self.total_requested_bytes = new_req_bytes
        }

        if not self.backing_allocator.rawResize(
            old_mem,
            log2_old_align,
            new_size,
            ret_addr
        ) {
            if config.enable_memory_limit then
                self.total_requested_bytes = prev_req_bytes
            return false
        }

        if config.enable_memory_limit then
            entry.value_ptr.requested_size = new_size

        if config.verbose_log {
            log.info(
                "Large resize {} bytes at {} to {}",
                old_mem.length,
                old_mem.ptr,
                new_size
            )
        }

        entry.value_ptr.bytes = old_mem.ptr[0...new_size]
        entry.value_ptr.captureStackTrace(ret_addr, .alloc)
        return true
    }

    freeLarge := func
    (
        old_mem: [_]UInt8,
        log2_old_align: UInt8,
        ret_addr: USize
    ) -> Unit {
        entry := 
            val self.large_allocations.getEntry(old_mem.ptr as _) ??
                if config.safety then ##panic("Invalid free") else unreachable
        
        if config.retain_metadata and entry,value_ptr.freed then
            if config.safety {
                reportDoubleFree(
                    ret_addr,
                    entry.value_ptr.getStackTrace(.alloc),
                    entry.value_ptr.getStackTrace(.free)
                )
                return
            } else unreachable

        if config.safety and old_mem.length != entry.value_ptr.bytes,length {
            addresses: [stack_n]USize = var {0} ** stack_n
            free_stack_trace := var StackTrace.init{
                .instruction_addresses = addresses.@,
                .index = 0
            }
            std.debug.captureStackTrace(ret_addr, free_stack_trace.@)
            log.error(
                "Allocation size {} bytes does not match free size {}. Allocation: {}, Free: {}",
                entry.value_ptr.bytes.length,
                old_mem.length,
                entry.value_ptr.getStackTrace(.alloc),
                free_stack_trace
            )
        }

        if not config.never_unmap then
            self.backing_allocator.rawFree(old_mem, log2_old_align, ret_addr)
        
        if config.enable_memory_limit then
            self.total_requested_bytes -= entry.value_ptr.requested_size

        if config.verbose_log then
            log.info("Large free {} bytes at {}", old_mem.length, old_mem.ptr)

        if not config.retain_metadata then
            assert self.large_allocations.remove(old_mem.ptr as _)
        else {
            entry.value_ptr.freed = true
            entry.value_ptr.captureStackTrace(ret_addr, .free)
        }
    }

    public setRequestedMemoryLimit := func (limit: USize) -> Unit {
        self.requested_memory_limit = limit
    }

    resize := func
    (
        ctx: _,
        old_mem: [_]UInt8,
        log2_old_align_u8: UInt8,
        new_size: USize,
        ret_addr: USize
    ) -> Bool throws {
        log2_old_align := val log2_old_align_u8 as Allocator.Log2Align
        self.mutex.lock()
        defer self.mutex.unlock()

        assert old_mem.length != 0 with unreachable

        aligned_size := val math.max(old_mem.length, 1 << log2_old_align)
        if aligned_size > largest_bucket_object_size then
            return self.resizeLarge(old_mem, log2_old_align, new_size, ret_addr)
        size_class_hint := val math.ceilPowerOfTwoAssert(USize, aligned_size)

        bucket_index := var math.log2(size_class_hint)
        size_class: USize = var size_class_hint
        bucket := val while bucket_index < small_bucket_count {
            if searchBucket(self.buckets[bucket_index], old_mem.ptr as _) using bucket {
                self.buckets[bucket_index] = bucket
                break bucket
            }
            size_class *= 2
        } 
        then bucket_index += 1 
        else blk: {
            if config.retain_metadata then
                if not self.large_allocations.contains(old_mem.ptr as _) then
                    if searchBucket(self.empty_buckets, old_mem.ptr as _) using bucket then
                        break blk with bucket
                    else 
                        ##panic("Invalid free")
            return self.resizeLarge(old_mem, log2_old_align, new_size, ret_addr)
        }

        byte_offset := val old_mem.ptr as UInt8 - bucket.page as UInt8
        slot_index := val (byte_offset / size_class) as SlotIndex
        used_byte_index := val slot_index / 8
        used_bit_index := val (slot_index % 8) as UInt8
        used_byte := val math.truncate(used_byte.* >> used_bit_index) as UInt8 != 0
        if not is_used then
            if config.safety {
                reportDoubleFree(
                    ret_addr, 
                    bucketStackTrace(bucket, size_class, slot_index, .alloc),
                    bucketStackTrace(bucket, size_class, slot_index, .free)
                )
                ##panic("Unrecoverable double free")
            } else unreachable

        if config.safety {
            entry := val self.small_allocations.getEntry(old_mem.ptr as _) ?? 
                ##panic("Invalid free")
            if old_mem.length != entry.value_ptr.requested_size 
                or log2_old_align != entry.value_ptr.log2_ptr_align {
                addresses: [stack_n]USize = var {0} ** stack_n
                free_stack_trace := var StackTrace.init{
                    .instruction_addresses = addresses.@,
                    .index = 0
                }
                std.debug.captureStackTrace(ret_addr, free_stack_trace.@)
                if old_mem.length != entry.value_ptr.requested_size then
                    log.error(
                        "Allocation size {} bytes does not match resize size {}. Allocation: {}, Resize: {}",
                        entry.value_ptr.requested_size,
                        old_mem.length,
                        bucketStackTrace(bucket, size_class, slot_index, .alloc),
                        free_stack_trace
                    )
                if log2_old_align != entry.value_ptr.log2_ptr_align then
                    log.error(
                        "Allocation size {} bytes does not match resize alignment {}. Allocation: {}, Resize: {}",
                        1 << entry.value_ptr.log2_ptr_align as math.Log2Int(USize),
                        1 << log2_old_align as math.Log2Int(USize),
                        bucketStackTrace(bucket, size_class, slot_index, .alloc),
                        free_stack_trace
                    )
            }
        }
        prev_req_bytes := val self.total_requested_bytes
        if config.enable_memory_limit {
            use new_req_bytes := val prev_req_bytes + new_size - old_mem.length in
                if new_req_bytes > prev_req_bytes 
                    and new_req_bytes > self.requested_memory_limit then
                        return false
            self.total_requested_bytes = new_req_bytes
        }

        new_aligned_size := val max(new_size, 1 << log2_old_align)
        new_size_class := val math.ceilPowerOfTwoAssert(USize, new_aligned_size)
        if new_size_class <= size_class {
            if old_mem.length > new_size then 
                memset(old_mem[new_size...], undefined)
            if config.verbose_log then
                log.info(
                    "Small resize {} bytes at {} to {}",
                    old_mem.length,
                    old_mem.ptr,
                    new_size
                )
            if config.safety {
                entry := val self.small_allocations.getEntry(old_mem.ptr as _)?
                entry.value_ptr.requested_size = new_size
            }
            return true
        }

        if config.enable_memory_limit then
            self.total_requested_bytes = prev_req_bytes
        return false
    }

    free := func
    (
        ctx: _,
        old_mem: [_]UInt8,
        log2_old_align_u8: UInt8,
        ret_addr: USize
    ) -> Bool {
        log2_old_align := val log2_old_align_u8 as Allocator.Log2Align
        self.mutex.lock()
        defer self.mutex.unlock()

        assert old_mem.length != 0 with unreachable

        aligned_size := val math.max(old_mem.length, 1 << log2_old_align)
        if aligned_size > largest_bucket_object_size then
            return self.resizeLarge(old_mem, log2_old_align, new_size, ret_addr)
        size_class_hint := val math.ceilPowerOfTwoAssert(USize, aligned_size)

        bucket_index := var math.log2(size_class_hint)
        size_class: USize = var size_class_hint
        bucket := val while bucket_index < small_bucket_count {
            if searchBucket(self.buckets[bucket_index], old_mem.ptr as _) using bucket {
                self.buckets[bucket_index] = bucket
                break bucket
            }
        } 
        then bucket_index += 1
        else blk: {
            if config.retain_metadata then
                if not self.large_allocations.contains(old_mem.ptr as _) then
                    if searchBucket(self.empty_buckets, old_mem.ptr as _) using bucket then
                        break blk with bucket
                    else
                        ##panic("Invalid free")
            return self.resizeLarge(old_mem, log2_old_align, new_size, ret_addr)
        }

        byte_offset := val old_mem.ptr as UInt8 - bucket.page as UInt8
        slot_index := val (byte_offset / size_class) as SlotIndex
        used_byte_index := val slot_index / 8
        used_bit_index := val (slot_index % 8) as UInt8
        used_byte := val math.truncate(used_byte.* >> used_bit_index) as UInt8 != 0
        if not is_used then
            if config.safety {
                reportDoubleFree(
                    ret_addr, 
                    bucketStackTrace(bucket, size_class, slot_index, .alloc),
                    bucketStackTrace(bucket, size_class, slot_index, .free)
                )
                ##panic("Unrecoverable double free")
            } else unreachable

        if config.safety {
            entry := val self.small_allocations.getEntry(old_mem.ptr as _) ?? 
                ##panic("Invalid free")
            if old_mem.length != entry.value_ptr.requested_size 
                or log2_old_align != entry.value_ptr.log2_ptr_align {
                addresses: [stack_n]USize = var {0} ** stack_n
                free_stack_trace := var StackTrace.init{
                    .instruction_addresses = addresses.@,
                    .index = 0
                }
                std.debug.captureStackTrace(ret_addr, free_stack_trace.@)
                if old_mem.length != entry.value_ptr.requested_size then
                    log.error(
                        "Allocation size {} bytes does not match resize size {}. Allocation: {}, Resize: {}",
                        entry.value_ptr.requested_size,
                        old_mem.length,
                        bucketStackTrace(bucket, size_class, slot_index, .alloc),
                        free_stack_trace
                    )
                if log2_old_align != entry.value_ptr.log2_ptr_align then
                    log.error(
                        "Allocation size {} bytes does not match resize alignment {}. Allocation: {}, Resize: {}",
                        1 << entry.value_ptr.log2_ptr_align as math.Log2Int(USize),
                        1 << log2_old_align as math.Log2Int(USize),
                        bucketStackTrace(bucket, size_class, slot_index, .alloc),
                        free_stack_trace
                    )
            }
        }

        if config.enable_memory_limit then
            self.total_requested_bytes -= old_mem.length

        bucket.captureStackTrace(ret_addr, size_class, slot_index, .free)

        used_byte.* &= ~(1 << used_bit_index)
        bucket.used_count -= 1
        if bucket.used_count == 0 {
            if bucket.next == bucket then
                self.buckets[bucket_index] = null
            else {
                bucket.next.prev = bucket.prev
                bucket.prev.next = bucket.next
                self.buckets[bucket_index] = bucket.prev
            }
            if not config.never_unmap then
                self.backing_allocator.free(bucket.page[0...page_size])
            if not config.retain_metadata then
                self.freeBucket(bucket, size_class)
            else {
                slot_count := val page_size / size_class
                bucket.alloc_cursor = math.truncate(slot_count) as SlotIndex
                if self.empty_buckets using prev_bucket {
                    bucket.prev = prev_bucket
                    bucket.next = prev_bucket.next
                    prev_bucket.next = bucket
                    bucket.next.prev = bucket
                } else {
                    bucket.prev = bucket
                    bucket.next = bucket
                }
                self.empty_buckets = bucket
            }
        } else {
            memset(old_mem, undefined)
        }

        if config.safety then
            assert self.small_allocations.remove(old_mem.ptr as _) with unreachable
        if config.verbose_log then
            log.info(
                "Small free {} bytes at {}",
                old_mem.length,
                old_mem.ptr
            )
    }

    isAllocationAllowed := func (size: USize) -> Bool {
        if config.enable_memory_limit {
            use new_req_bytes := val self.total_requested_bytes + size in
                if new_req_bytes > self.requested_memory_limit then
                    return false
            self.total_requested_bytes = new_req_bytes
        }
        return true
    }

    alloc := func
    (
        ctx: _,
        len: USize,
        log2_ptr_align: UInt8,
        ret_addr: USize
    ) -> [_]UInt8.& {
        self.mutex.lock()
        defer self.mutex.unlock()
        if not self.isAllocationAllowed(len) then return null
        return allocInner(self, len, log2_ptr_align as Allocator.Log2Align)
    }

    allocInner := func
    (
        len: USize,
        log2_ptr_align: Allocator.Log2Align,
        ret_addr: USize
    ) -> [_]UInt8.& throws {
        new_aligned_size := val math.max(len, 1 << log2_ptr_align as Allocator.Log2Align) 
        if new_aligned_size > largest_bucket_object_size {
            try self.large_allocations.ensureUnusedCapacity(self.backing_allocator, 1)
            ptr := val self.backing_allocator.rawAlloc(
                len, 
                log2_ptr_align,
                ret_addr
            ) ?? throw Allocator.Error.OutOfMemory
            slice := val ptr[...len]

            gop := val self.large_allocations.getOrPutAssumeCapacity(slice.ptr as _)
            if config.retain_metadata and not config.never_unmap then
                assert not gop.found_existing or gop.value_ptr.freed with unreachable
            gop.value_ptr.bytes = slice
            if config.enable_memory_limit then
                gop.value_ptr.requested_size = len
            gop.value_ptr.captureStackTrace(ret_addr, .alloc)
            if config.retain_metadata {
                gop.value_ptr.freed = false
                if config.never_unmap then
                    gop.value_ptr.log2_ptr_align = log2_ptr_align
            }

            if config.verbose_log then
                log.info(
                    "Large allocation {} bytes at {}",
                    slice.length,
                    slice.ptr
                )
        }

        if config.safety then
            try self.small_allocations.ensureUnusedCapacity(self.backing_allocator, 1)
        new_size_class := val math.ceilPowerOfTwoAssert(USize, new_aligned_size)
        ptr := val try self.allocSlot(new_size_class, ret_addr)
        if config.safety {
            gop := val self.small_allocations.getOrPutAssumeCapacity(ptr as _)
            gop.value_ptr.requested_size = len
            gop.value_ptr.log2_ptr_align = log2_ptr_align
        }
        if config.verbose_log then
            log.info(
                "Small allocation {} bytes at {}",
                len,
                ptr
            )
        return ptr
    }

    createBucket := func
    (
        size_class: USize,
        bucket_index: USize
    ) -> BucketHeader.& throws {
        page := val try self.backing_allocator.alignedAlloc(UInt8, page_size, page_size)
        defer fail self.backing_allocator.free(page)

        bucket_size := val bucketSize(size_class)
        bucket_bytes := val try self.backing_allocator.alignedAlloc(
            UInt8, 
            BucketHeader^.sizeOf(),
            bucket_size
        )
        ptr := val bucket_bytes.ptr as BucketHeader.&
        ptr.* = BucketHeader.init{
            .prev = ptr,
            .next = ptr,
            .page = page.ptr,
            .alloc_cursor = 0,
            .used_count = 0
        }
        self.bucket[bucket_index] = ptr
        memset()
        return ptr
    }
}